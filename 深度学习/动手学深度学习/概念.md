机器学习的**目标**是发现一种泛化的模式（可以在训练集之外的数据中应用）。

多模态 对齐

模型存在智能上限，当增加训练数据和参数时，无法提升模型性能。

深度网络的泛化性质令人费解，而这种泛化性质的数学基础仍然是悬而未决的研究问题。

训练过程的**前向传播**和**后向传播**有严格的数学证明。

**softmax：**$\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}$

**激活函数：** 将线性模型转为非线性模型的关键。ReLU，pReLU，sigmoid，tanh

**损失函数（目标函数）：** 用来量化模型的有效性。（预测损失+惩罚项（如L2正则化增加权重项））
- **均方差损失**：高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。
- **交叉熵损失**：适用于均匀分布的one-hot编码
$$ l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j. $$

模型在训练数据上拟合的比在潜在分布中更接近的现象称为**过拟合**（overfitting）， 用于对抗过拟合的技术称为**正则化**（regularization）

在训练参数化机器学习模型时， **权重衰减**（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为L2正则化。 

**训练误差**（training error）是指， 模型在训练数据集上计算得到的误差。 **泛化误差**（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。我们永远不能准确地计算出泛化误差。 这是因为无限多的数据样本是一个虚构的对象。 在实际中，我们只能通过将模型应用于一个独立的测试集来估计泛化误差， 该测试集由随机选取的、未曾在训练集中出现的数据样本构成。

假设训练数据和测试数据都是从相同的分布中独立提取的。 这通常被称为*独立同分布假设**（i.i.d. assumption）

是否过拟合或欠拟合可能取决于模型复杂性和可用训练数据集的大小

![](_attachments/Pasted%20image%2020230823213832.png)
训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。 随着训练数据量的增加，泛化误差通常会减小。

梯度消失和梯度爆炸 [出现梯度消失与梯度爆炸的原因以及解决方案 - 控球强迫症 - 博客园 (cnblogs.com)](https://www.cnblogs.com/XDU-Lakers/p/10553239.html)

权重衰减： 如L2正则化

**暂退法（Dropout）**:
**暂退法**在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。 这种方法之所以被称为*暂退法*，因为我们从表面上看是在训练过程中丢弃（drop out）一些神经元。 在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。通常，我们在测试时不用暂退法。 给定一个训练好的模型和一个新的样本，我们不会丢弃任何节点，因此不需要标准化。 然而也有一些例外：一些研究人员在测试时使用暂退法， 用于估计神经网络预测的“不确定性”： 如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。
- 暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。
- 暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。
- 暂退法将活性值替换为具有期望值的随机变量。
- 暂退法仅在训练期间使用。
  
**反向传播**（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。

在训练神经网络时，前向传播和反向传播相互依赖。 对于前向传播，我们沿着依赖的方向遍历计算图并计算其路径上的所有变量。 然后将这些用于反向传播，其中计算顺序与计算图的相反。

**参数初始化**
- 当多个神经元或隐藏单元以完全相同的方式初始化时，它们在训练过程中将共同“学习”，并始终具有相同的行为。这样，多个神经元就会像一个神经元那样行为，从而大大限制了网络的容量和表达能力。解决这个问题的常用方法是在初始化时为神经网络的权重添加随机性。这样，每个隐藏单元从一开始就有了微小的差异，这些差异会在训练过程中被放大，从而打破对称性。这就是为什么权重通常使用例如小的随机数来初始化，而不是全零或其他恒定值。
- Xavier初始化

**环境和分布偏移**
- 虽然输入的分布可能随时间而改变， 但标签函数（即条件分布）没有改变。 统计学家称之为*协变量偏移*（covariate shift）， 因为这个问题是由于协变量（特征）分布的变化而产生的。

多模态大模型

Clip: 解决了图片和文本分类问题

图片和文本对齐

Blip: 同时解决了分类和生成的问题

解决了弱监督图文数据的清洗问题

